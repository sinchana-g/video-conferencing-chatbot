<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chatbot with Voice</title>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <style>
    body {
        font-family: Arial, sans-serif;
        background-color: #f4f4f4;
        margin: 0;
        display: flex;
        flex-direction: column;
        align-items: center;
        height: 100vh;
    }

    /* Video container */
    .video-container {
        width: 100%;
        display: flex;
        justify-content: center;
        align-items: center;
        margin-top: 20px;
        margin-bottom: 20px;
        flex-wrap: nowrap; /* Keeps items from wrapping to the next line */
    }

    /* Candidate video */
    #candidate-video {
        width: 400px; /* Adjust width */
        height: 300px; /* Adjust height */
        border: 2px solid #ddd;
        border-radius: 8px;
        background-color: black;
        margin: 10px;
        transform: scaleX(-1);
    }

    /* 3D avatar container */
    #three-d-avatar-container {
        width: 400px; /* Adjust width */
        height: 300px; /* Adjust height */
        border: 2px solid #ddd;
        border-radius: 8px;
        background-color: black;
        margin: 10px;
    }



    /* Chat container */
    .chat-container {
        width: 400px;
        max-width: 100%;
        background: white;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        overflow: hidden;
        margin-top: 20px;
        display: flex;
        flex-direction: column;
    }

    .chat-box {
        height: 400px;
        overflow-y: auto;
        padding: 10px;
        display: flex;
        flex-direction: column;
    }

    .message {
        padding: 10px;
        border-radius: 8px;
        margin: 5px;
        max-width: 80%;
        word-wrap: break-word;
    }

    .user-message {
        background: #007bff;
        color: white;
        align-self: flex-end;
    }

    .bot-message {
        background: #e9ecef;
        align-self: flex-start;
    }

    .input-container {
        display: flex;
        padding: 10px;
        border-top: 1px solid #ddd;
        background: white;
    }

    input {
        flex: 1;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px;
        outline: none;
    }

    button {
        margin-left: 10px;
        padding: 10px 15px;
        background: #007bff;
        color: white;
        border: none;
        border-radius: 5px;
        cursor: pointer;
    }

    button:hover {
        background: #0056b3;
    }

    .loading {
        font-size: 12px;
        color: #888;
        font-style: italic;
    }
</style>
</head>
<body>
    <!-- Video container for candidate and interviewer -->
    <div class="video-container">
        <div id="three-d-avatar-container"></div>
        <video id="candidate-video" autoplay playsinline></video>
    </div>


    <!-- Chat container for the chatbot -->
    <div class="chat-container">
        <div class="chat-box" id="chat-box"></div>
        <div class="input-container">
            <input type="text" id="user-input" placeholder="Type your message..." autofocus>
            <button onclick="sendMessage()">Send</button>
        </div>
    </div>

    <div id="emotion-display" style="text-align:center; font-size:20px; margin-top:10px;"></div>


    <audio id="audio-player" controls style="display:none;"></audio>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.3.2/socket.io.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/build/three.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/gsap.min.js"></script>



    <script>
    // Initialize Socket.io
    const socket = io.connect("http://127.0.0.1:5000");

    let localPeerConnection;
    let remotePeerConnection;
    let localStream;
    let remoteStream;

    let scene, camera, renderer, avatar;
    let head;

    function init3DAvatar() {
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        renderer = new THREE.WebGLRenderer();


        // scene.background = new THREE.Color(0xFFB6C1);

        // Set a custom background image for the scene
        const texture_loader = new THREE.TextureLoader();
        const texture = texture_loader.load('static/background3.jpg'); // Your background image path
        scene.background = texture;

        // Position the camera closer to the face
        camera.position.set(0, 1.7, 2); // Adjust the z-value to zoom in closer to the face

        // Optional: Adjust FOV to make it feel more zoomed in
        camera.fov = 8; // Lower value zooms in, higher value zooms out
        camera.updateProjectionMatrix(); // Update projection matrix after changing FOV
        
        const avatarContainer = document.getElementById('three-d-avatar-container');
        camera.aspect = avatarContainer.offsetWidth / avatarContainer.offsetHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(avatarContainer.offsetWidth, avatarContainer.offsetHeight);
        avatarContainer.appendChild(renderer.domElement);  // Append to the new container

        // Add lighting to the scene
        const light = new THREE.AmbientLight(0x404040, 5);  // Ambient light
        scene.add(light);
        
        const directionalLight = new THREE.DirectionalLight(0xffffff, 1);  // Directional light
        directionalLight.position.set(1, 1, 1).normalize();
        scene.add(directionalLight);

        // Load the GLB avatar model using THREE.GLTFLoader
        const loader = new THREE.GLTFLoader();
        loader.load('http://127.0.0.1:5000/static/67dc0b52b7bad2cc2ac55a7e.glb', function (gltf) {
            avatar = gltf.scene;
            avatar.scale.set(1, 1, 1);  // Adjust the avatar scale
            avatar.position.set(0, 0, 0);  // Position the avatar in the scene
            scene.add(avatar);
            avatar.traverse((child) => {
                // console.log(child.name);  // Print names of all parts
                if (child.name.toLowerCase().includes("head")) {
                    head = child;  // Assign head if found
                }
                if (child.isMesh && child.morphTargetDictionary) {
                    console.log(child.name, child.morphTargetDictionary);
                }
            });

        }, undefined, function (error) {
            console.error("Error loading GLB model:", error);
        });

        camera.position.z = 3;  // Adjust camera position
    }

    function animateMouth(intensity) {
        avatar.traverse((child) => {
            if (child.isMesh && child.morphTargetDictionary && child.name === "Wolf3D_Head") {
                if ("mouthOpen" in child.morphTargetDictionary) {
                    const openIndex = child.morphTargetDictionary["mouthOpen"];
                    child.morphTargetInfluences[openIndex] = intensity;
                }
                if ("mouthSmile" in child.morphTargetDictionary) {
                    const smileIndex = child.morphTargetDictionary["mouthSmile"];
                    child.morphTargetInfluences[smileIndex] = intensity * 0.3; // Adjust smile effect
                }
            }
        });
    }

    function syncMouthWithAudio() {
        if (!avatar) return;
        
        if (!audioPlayer.paused) {
            let mouthIntensity = Math.abs(Math.sin(performance.now() * 0.005)); // Smooth open/close
            animateMouth(mouthIntensity);
            requestAnimationFrame(syncMouthWithAudio);
        } else {
            animateMouth(0); // Keep mouth closed when no audio
        }
    }


    function animate() {
        requestAnimationFrame(animate);
        // Rotate the avatar for animation (optional)
        // if (avatar) {
        //     avatar.rotation.y += 0.01;
        // }

        renderer.render(scene, camera);
    }



    // Access the user's webcam
    async function startWebcam() {
        try {
            // Stop any existing audio tracks before starting new ones
            if (localStream) {
                localStream.getAudioTracks().forEach(track => track.stop());
            }

            // Request new media stream with echo cancellation
            localStream = await navigator.mediaDevices.getUserMedia({
                video: true,
                audio: { echoCancellation: true, noiseSuppression: true },
            });

            const videoElement = document.getElementById('candidate-video');
            videoElement.srcObject = localStream;

            localPeerConnection = new RTCPeerConnection();
            localStream.getTracks().forEach(track => {
                localPeerConnection.addTrack(track, localStream);
            });

            localPeerConnection.onicecandidate = event => {
                if (event.candidate) {
                    socket.emit('ice-candidate', event.candidate);
                }
            };

            remotePeerConnection = new RTCPeerConnection();
            remotePeerConnection.ontrack = event => {
                remoteStream = event.streams[0];
                const remoteVideo = document.getElementById('interviewer-video');
                remoteVideo.srcObject = remoteStream;
                document.getElementById('interviewer-video').muted = true;  // Mute interviewer output
            };

        } catch (error) {
            console.error('Error accessing webcam: ', error);
        }
    }


    // Start webcam
    startWebcam();

    async function captureFrame() {
        const videoElement = document.getElementById('candidate-video');
        const canvas = document.createElement('canvas');
        const context = canvas.getContext('2d');
        
        // Draw the current frame from the video element to the canvas
        context.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
        
        // Convert the canvas image to base64
        const imageData = canvas.toDataURL('image/jpeg');
        
        // Send the base64 image to the backend for emotion analysis
        $.ajax({
            url: '/analyze_emotion',
            type: 'POST',
            contentType: 'application/json',
            data: JSON.stringify({ image: imageData }),
            success: function(response) {
                // Display detected emotion on the page
                const emotionDisplay = document.getElementById('emotion-display');
                emotionDisplay.innerHTML = `Detected Emotion: ${response.emotion}`;
            },
            error: function(err) {
                console.log('Error analyzing emotion:', err);
            }
        });
    }

    // Capture frames every 500ms (adjust as needed)
    setInterval(captureFrame, 500);


    function updateEmotionDisplay(emotion) {
        console.log("Emotion received:", emotion);
        const emotionText = document.getElementById("emotion-display");
        emotionText.innerText = `Emotion: ${emotion}`;
    }


    

    // Global recognition instance
    var recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = "en-US";
    recognition.continuous = true;
    recognition.interimResults = false;

    recognition.onresult = function(event) {
        var transcript = event.results[event.results.length - 1][0].transcript;
        $("#user-input").val(transcript);
        sendMessage();
    };

    recognition.onerror = function(event) {
        console.error("Speech recognition error:", event.error);
    };

    function startListening() {
        if (recognition) {
            recognition.start();
        }
    }

    function stopRecognition() {
        if (recognition) {
            recognition.onend = null; // Prevent auto-restart
            recognition.abort();
        }
    }

    // Ensure recognition restarts only after TTS playback
    var audioPlayer = document.getElementById("audio-player");
    audioPlayer.onended = function() {
        animateMouth(0);  // Ensure mouth fully closes
        startListening();  // Resume listening after TTS finishes

        // ðŸŽ¤ Restart microphone stream
        navigator.mediaDevices.getUserMedia({ audio: true, video: false })
            .then(stream => {

                // Remove any previous tracks
                localStream.getAudioTracks().forEach(track => track.stop());
                localStream = stream;

                // Add the new audio track to peer connection
                localPeerConnection.getSenders().forEach(sender => {
                    if (sender.track.kind === 'audio') {
                        sender.replaceTrack(stream.getAudioTracks()[0]);
                    }
                });

            })
            .catch(error => console.error("Error restarting microphone:", error));
    };



    // Modify sendMessage to stop recognition before TTS
    function sendMessage() {
        var userInput = $("#user-input").val().trim();
        if (userInput === "") return;

        $("#chat-box").append(`<div class="message user-message">${userInput}</div>`);
        $("#chat-box").append(`<div class="message bot-message loading">Typing...</div>`);
        $("#chat-box").scrollTop($("#chat-box")[0].scrollHeight);

        $.ajax({
            type: "POST",
            url: "/ask",
            data: { user_input: userInput },
            success: function(response) {
                $(".loading").remove();
                $("#chat-box").append(`<div class="message bot-message">${response.response}</div>`);
                $("#chat-box").scrollTop($("#chat-box")[0].scrollHeight);

                var audioPlayer = document.getElementById("audio-player");

                // Stop recognition before playing TTS
                stopRecognition();

                // REMOVE OLD AUDIO TRACKS BEFORE PLAYING NEW AUDIO
                localStream.getAudioTracks().forEach(track => {
                    track.stop();
                    localStream.removeTrack(track);
                });

                // Set and play the new TTS response
                audioPlayer.src = response.audio + "?t=" + new Date().getTime();
                audioPlayer.style.display = "block";
                audioPlayer.volume = 0.5;  // Reduce volume slightly
                // audioPlayer.muted = true;  // Mute if you only need text feedback

                audioPlayer.play();
                syncMouthWithAudio(); // Start syncing mouth when audio plays

                audioPlayer.onended = function() {
                    startListening();
                }
            }
        });

        $("#user-input").val("");
    }



    // Start listening when page loads
    startListening();


    $("#user-input").keypress(function(event) {
        if (event.which === 13) {
            sendMessage();
        }
    });


    init3DAvatar();
    animate();



</script>



</body>
</html>